<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Smart Doc Voice Assistant - Hands Free</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
            -webkit-tap-highlight-color: transparent;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #000;
            color: #fff;
            height: 100vh;
            display: flex;
            flex-direction: column;
            overflow: hidden;
            touch-action: manipulation;
        }
        
        .header {
            background: #111;
            padding: 15px;
            text-align: center;
            border-bottom: 1px solid #333;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }
        
        .mode-indicator {
            background: #2ecc71;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 14px;
        }
        
        .chat-container {
            flex: 1;
            overflow-y: auto;
            padding: 20px;
            display: flex;
            flex-direction: column;
            gap: 15px;
            -webkit-overflow-scrolling: touch;
        }
        
        .message {
            max-width: 85%;
            padding: 15px 20px;
            border-radius: 20px;
            word-wrap: break-word;
            animation: fadeIn 0.3s ease-in;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        .user-message {
            align-self: flex-end;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin-left: auto;
        }
        
        .assistant-message {
            align-self: flex-start;
            background: #1a1a1a;
            border: 1px solid #333;
            color: #e0e0e0;
        }
        
        .system-message {
            align-self: center;
            background: none;
            color: #888;
            font-style: italic;
            font-size: 14px;
            padding: 10px;
        }
        
        .controls {
            background: #111;
            border-top: 1px solid #333;
            padding: 30px;
            text-align: center;
        }
        
        .mic-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            background: #333;
            border: 3px solid #555;
            color: white;
            font-size: 50px;
            cursor: pointer;
            transition: all 0.3s;
            position: relative;
            touch-action: manipulation;
            -webkit-user-select: none;
            user-select: none;
        }
        
        .mic-button:active {
            transform: scale(0.95);
        }
        
        .mic-button.listening {
            background: #e74c3c;
            border-color: #c0392b;
            animation: pulse 1.5s infinite;
        }
        
        .mic-button.processing {
            background: #f39c12;
            border-color: #d68910;
        }
        
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(231, 76, 60, 0.7); }
            70% { box-shadow: 0 0 0 30px rgba(231, 76, 60, 0); }
            100% { box-shadow: 0 0 0 0 rgba(231, 76, 60, 0); }
        }
        
        .status {
            margin-top: 20px;
            font-size: 18px;
            color: #aaa;
        }
        
        .silence-indicator {
            margin-top: 10px;
            height: 4px;
            background: #333;
            border-radius: 2px;
            overflow: hidden;
            display: none;
        }
        
        .silence-progress {
            height: 100%;
            background: #3498db;
            width: 0%;
            transition: width 0.1s linear;
        }
        
        .interrupt-note {
            position: fixed;
            bottom: 180px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0,0,0,0.8);
            padding: 10px 20px;
            border-radius: 20px;
            font-size: 14px;
            display: none;
        }
        
        .audio-visualizer {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            display: flex;
            gap: 3px;
            height: 40px;
            align-items: center;
        }
        
        .audio-bar {
            width: 4px;
            background: white;
            transition: height 0.1s;
        }
    </style>
</head>
<body>
    <div class="header">
        <h2>Smart Doc Assistant</h2>
        <div class="mode-indicator">Hands-Free Mode</div>
    </div>
    
    <div class="chat-container" id="chatContainer">
        <div class="message assistant-message">
            Hey Jeff, how's it going? Just tap the microphone when you're ready to talk. I'll stop listening after you pause.
        </div>
    </div>
    
    <div class="interrupt-note" id="interruptNote">
        Tap to interrupt
    </div>
    
    <div class="controls">
        <button class="mic-button" id="micButton" type="button">
            <span id="micIcon">🎤</span>
            <div class="audio-visualizer" id="visualizer" style="display: none;">
                <div class="audio-bar" style="height: 10px;"></div>
                <div class="audio-bar" style="height: 20px;"></div>
                <div class="audio-bar" style="height: 15px;"></div>
                <div class="audio-bar" style="height: 25px;"></div>
                <div class="audio-bar" style="height: 18px;"></div>
            </div>
        </button>
        <div class="status" id="status">Tap to speak</div>
        <div class="silence-indicator" id="silenceIndicator">
            <div class="silence-progress" id="silenceProgress"></div>
        </div>
    </div>
    
    <script>
        // NEW APPROACH: Using Web Audio API Worklet for recording and analysis
        let audioContext = null;
        let mediaStream = null;
        let workletNode = null;
        let isListening = false;
        let isProcessing = false;
        let currentAudio = null;
        let conversationHistory = [];
        let audioChunks = [];
        let silenceDuration = 0;
        
        const micButton = document.getElementById('micButton');
        const micIcon = document.getElementById('micIcon');
        const visualizer = document.getElementById('visualizer');
        const status = document.getElementById('status');
        const chatContainer = document.getElementById('chatContainer');
        const silenceIndicator = document.getElementById('silenceIndicator');
        const silenceProgress = document.getElementById('silenceProgress');
        const interruptNote = document.getElementById('interruptNote');
        
        // Create the worklet processor code
        const processorCode = `
            class RecorderProcessor extends AudioWorkletProcessor {
                constructor() {
                    super();
                    this.isRecording = false;
                    this.bufferSize = 4096;
                    this.buffer = [];
                }
                
                process(inputs, outputs, parameters) {
                    const input = inputs[0];
                    if (input.length > 0) {
                        const inputData = input[0];
                        
                        // Calculate audio level
                        let sum = 0;
                        for (let i = 0; i < inputData.length; i++) {
                            sum += Math.abs(inputData[i]);
                        }
                        const average = sum / inputData.length;
                        
                        // Send level to main thread
                        this.port.postMessage({
                            type: 'level',
                            level: average * 1000 // Scale up for visibility
                        });
                        
                        // Record if active
                        if (this.isRecording) {
                            // Copy the audio data
                            const copy = new Float32Array(inputData);
                            this.buffer.push(copy);
                            
                            // Send chunks periodically
                            if (this.buffer.length >= 10) {
                                this.port.postMessage({
                                    type: 'audio',
                                    buffer: this.buffer
                                });
                                this.buffer = [];
                            }
                        }
                    }
                    
                    return true;
                }
            }
            
            registerProcessor('recorder-processor', RecorderProcessor);
        `;
        
        // Initialize audio with worklet approach
        async function initAudio() {
            try {
                if (audioContext) return;
                
                console.log('Initializing audio with Worklet approach...');
                status.textContent = 'Getting microphone...';
                
                // Get microphone access
                mediaStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });
                
                // Create audio context
                audioContext = new AudioContext();
                
                // Create worklet
                const blob = new Blob([processorCode], { type: 'application/javascript' });
                const workletUrl = URL.createObjectURL(blob);
                await audioContext.audioWorklet.addModule(workletUrl);
                
                // Create nodes
                const source = audioContext.createMediaStreamSource(mediaStream);
                workletNode = new AudioWorkletNode(audioContext, 'recorder-processor');
                
                // Handle messages from worklet
                let audioLevel = 0;
                workletNode.port.onmessage = (e) => {
                    if (e.data.type === 'level') {
                        audioLevel = e.data.level;
                        
                        // Update visualizer
                        if (isListening) {
                            updateVisualizer(audioLevel);
                            checkSilence(audioLevel);
                        }
                        
                        // Check for interruption
                        if (currentAudio && !currentAudio.paused && audioLevel > 30) {
                            currentAudio.pause();
                            currentAudio = null;
                            interruptNote.style.display = 'none';
                            status.textContent = 'Interrupted - tap to speak';
                        }
                    } else if (e.data.type === 'audio' && isListening) {
                        // Collect audio chunks
                        e.data.buffer.forEach(chunk => audioChunks.push(chunk));
                    }
                };
                
                // Connect the audio graph
                source.connect(workletNode);
                
                console.log('Audio initialized with Worklet');
                status.textContent = 'Microphone ready';
                
                // Start the worklet
                startWorklet();
                
            } catch (err) {
                console.error('Audio init failed:', err);
                status.textContent = 'Microphone access required';
                micButton.disabled = true;
            }
        }
        
        function startWorklet() {
            if (workletNode) {
                workletNode.port.postMessage({ command: 'start' });
            }
        }
        
        function updateVisualizer(level) {
            const bars = visualizer.querySelectorAll('.audio-bar');
            bars.forEach((bar, i) => {
                const height = Math.min(40, level * (i + 1) / 5);
                bar.style.height = height + 'px';
            });
        }
        
        const SILENCE_THRESHOLD = 5;
        const SILENCE_DURATION = 2000;
        
        function checkSilence(level) {
            if (level < SILENCE_THRESHOLD) {
                silenceDuration += 16;
                const progress = Math.min(100, (silenceDuration / SILENCE_DURATION) * 100);
                silenceProgress.style.width = progress + '%';
                
                if (silenceDuration >= SILENCE_DURATION) {
                    console.log('Silence detected, stopping...');
                    stopListening();
                }
            } else {
                silenceDuration = 0;
                silenceProgress.style.width = '0%';
            }
        }
        
        // Handle mic button
        async function handleMicPress() {
            if (isProcessing) return;
            
            // Initialize on first press
            if (!audioContext) {
                await initAudio();
            }
            
            if (currentAudio) {
                currentAudio.pause();
                currentAudio = null;
                interruptNote.style.display = 'none';
                addMessage('(Interrupted)', 'system-message');
            }
            
            if (isListening) {
                stopListening();
            } else {
                startListening();
            }
        }
        
        micButton.addEventListener('click', handleMicPress);
        micButton.addEventListener('touchend', (e) => {
            e.preventDefault();
            handleMicPress();
        });
        
        function startListening() {
            console.log('Starting listening...');
            audioChunks = [];
            silenceDuration = 0;
            
            // Tell worklet to start recording
            if (workletNode) {
                workletNode.port.postMessage({ command: 'record', value: true });
            }
            
            // Beep to indicate start
            try {
                const beep = new AudioContext();
                const oscillator = beep.createOscillator();
                oscillator.connect(beep.destination);
                oscillator.frequency.value = 800;
                oscillator.start();
                oscillator.stop(beep.currentTime + 0.1);
            } catch (e) {
                console.log('Beep failed:', e);
            }
            
            isListening = true;
            micButton.classList.add('listening');
            micIcon.style.display = 'none';
            visualizer.style.display = 'flex';
            status.textContent = 'Listening... (stops on pause)';
            silenceIndicator.style.display = 'block';
        }
        
        function stopListening() {
            console.log('Stopping listening...');
            
            if (workletNode) {
                workletNode.port.postMessage({ command: 'record', value: false });
            }
            
            isListening = false;
            micButton.classList.remove('listening');
            micButton.classList.add('processing');
            micIcon.style.display = 'block';
            micIcon.textContent = '⏳';
            visualizer.style.display = 'none';
            status.textContent = 'Processing...';
            silenceIndicator.style.display = 'none';
            silenceProgress.style.width = '0%';
            
            // Process the audio
            if (audioChunks.length > 0) {
                processRecordedAudio();
            } else {
                resetButton();
            }
        }
        
        async function processRecordedAudio() {
            try {
                // Convert Float32Array chunks to WAV
                const sampleRate = audioContext.sampleRate;
                const length = audioChunks.reduce((acc, chunk) => acc + chunk.length, 0);
                const buffer = new Float32Array(length);
                let offset = 0;
                
                audioChunks.forEach(chunk => {
                    buffer.set(chunk, offset);
                    offset += chunk.length;
                });
                
                // Convert to WAV
                const wavBlob = float32ToWav(buffer, sampleRate);
                console.log('Created WAV blob:', wavBlob.size);
                
                // Send to server
                const formData = new FormData();
                formData.append('audio', wavBlob, 'recording.wav');
                
                const response = await fetch('/api/voice/transcribe', {
                    method: 'POST',
                    body: formData
                });
                
                const result = await response.json();
                console.log('Transcription result:', result);
                
                if (result.success && result.text) {
                    // Filter hallucinations
                    const text = result.text.trim();
                    const hallucinations = ['you', 'thank you', 'thanks for watching', 'bye', 'bye-bye', '.', ''];
                    
                    if (hallucinations.includes(text.toLowerCase())) {
                        status.textContent = 'No speech detected - tap to try again';
                        resetButton();
                        return;
                    }
                    
                    addMessage(text, 'user-message');
                    
                    // Get AI response
                    const thinkingMsg = addMessage('G is thinking...', 'system-message');
                    
                    const aiResponse = await fetch('/api/chat', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({
                            message: text,
                            history: conversationHistory
                        })
                    });
                    
                    const aiResult = await aiResponse.json();
                    
                    if (thinkingMsg.parentNode) {
                        thinkingMsg.remove();
                    }
                    
                    if (aiResult.success && aiResult.response) {
                        conversationHistory.push({user: text, assistant: aiResult.response});
                        addMessage(aiResult.response, 'assistant-message');
                        
                        // Play response
                        const ttsResponse = await fetch('/api/voice/speak', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify({ text: aiResult.response })
                        });
                        
                        const ttsResult = await ttsResponse.json();
                        
                        if (ttsResult.success && ttsResult.audio_url) {
                            currentAudio = new Audio(ttsResult.audio_url);
                            interruptNote.style.display = 'block';
                            
                            currentAudio.onended = () => {
                                currentAudio = null;
                                interruptNote.style.display = 'none';
                                resetButton();
                            };
                            
                            currentAudio.onerror = () => {
                                currentAudio = null;
                                interruptNote.style.display = 'none';
                                resetButton();
                            };
                            
                            await currentAudio.play();
                        } else {
                            resetButton();
                        }
                    } else {
                        status.textContent = 'Error getting response';
                        resetButton();
                    }
                } else {
                    status.textContent = 'Error: ' + (result.error || 'No transcription');
                    resetButton();
                }
                
            } catch (error) {
                console.error('Processing error:', error);
                status.textContent = 'Error: ' + error.message;
                resetButton();
            }
        }
        
        function float32ToWav(buffer, sampleRate) {
            const length = buffer.length;
            const arrayBuffer = new ArrayBuffer(44 + length * 2);
            const view = new DataView(arrayBuffer);
            
            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };
            
            writeString(0, 'RIFF');
            view.setUint32(4, 36 + length * 2, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(36, 'data');
            view.setUint32(40, length * 2, true);
            
            // Convert float32 to int16
            let offset = 44;
            for (let i = 0; i < length; i++) {
                const sample = Math.max(-1, Math.min(1, buffer[i]));
                view.setInt16(offset, sample * 0x7FFF, true);
                offset += 2;
            }
            
            return new Blob([arrayBuffer], { type: 'audio/wav' });
        }
        
        function resetButton() {
            console.log('Resetting button state');
            isProcessing = false;
            isListening = false;
            micButton.classList.remove('processing', 'listening');
            micIcon.style.display = 'block';
            micIcon.textContent = '🎤';
            visualizer.style.display = 'none';
            status.textContent = 'Tap to speak';
            silenceIndicator.style.display = 'none';
            silenceProgress.style.width = '0%';
            audioChunks = [];
        }
        
        function addMessage(text, className) {
            const messageDiv = document.createElement('div');
            messageDiv.className = 'message ' + className;
            messageDiv.textContent = text;
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
            return messageDiv;
        }
        
        // Handle page visibility
        document.addEventListener('visibilitychange', () => {
            if (document.hidden && isListening) {
                stopListening();
            }
            if (document.hidden && currentAudio) {
                currentAudio.pause();
            }
        });
        
        // Prevent double-tap zoom
        let lastTouchEnd = 0;
        document.addEventListener('touchend', (e) => {
            const now = Date.now();
            if (now - lastTouchEnd <= 300) {
                e.preventDefault();
            }
            lastTouchEnd = now;
        }, false);
        
        // Wait for user interaction to init (iOS requirement)
        console.log('Ready - tap microphone to start');
    </script>
</body>
</html>