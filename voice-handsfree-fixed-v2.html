<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Smart Doc Voice Assistant - Hands Free</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
            -webkit-tap-highlight-color: transparent;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #000;
            color: #fff;
            height: 100vh;
            display: flex;
            flex-direction: column;
            overflow: hidden;
            touch-action: manipulation;
        }
        
        .header {
            background: #111;
            padding: 15px;
            text-align: center;
            border-bottom: 1px solid #333;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }
        
        .mode-indicator {
            background: #2ecc71;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 14px;
        }
        
        .chat-container {
            flex: 1;
            overflow-y: auto;
            padding: 20px;
            display: flex;
            flex-direction: column;
            gap: 15px;
            -webkit-overflow-scrolling: touch;
        }
        
        .message {
            max-width: 85%;
            padding: 15px 20px;
            border-radius: 20px;
            word-wrap: break-word;
            animation: fadeIn 0.3s ease-in;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        .user-message {
            align-self: flex-end;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin-left: auto;
        }
        
        .assistant-message {
            align-self: flex-start;
            background: #1a1a1a;
            border: 1px solid #333;
            color: #e0e0e0;
        }
        
        .system-message {
            align-self: center;
            background: none;
            color: #888;
            font-style: italic;
            font-size: 14px;
            padding: 10px;
        }
        
        .controls {
            background: #111;
            border-top: 1px solid #333;
            padding: 30px;
            text-align: center;
        }
        
        .mic-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            background: #333;
            border: 3px solid #555;
            color: white;
            font-size: 50px;
            cursor: pointer;
            transition: all 0.3s;
            position: relative;
            touch-action: manipulation;
            -webkit-user-select: none;
            user-select: none;
        }
        
        .mic-button:active {
            transform: scale(0.95);
        }
        
        .mic-button.listening {
            background: #e74c3c;
            border-color: #c0392b;
            animation: pulse 1.5s infinite;
        }
        
        .mic-button.processing {
            background: #f39c12;
            border-color: #d68910;
        }
        
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(231, 76, 60, 0.7); }
            70% { box-shadow: 0 0 0 30px rgba(231, 76, 60, 0); }
            100% { box-shadow: 0 0 0 0 rgba(231, 76, 60, 0); }
        }
        
        .status {
            margin-top: 20px;
            font-size: 18px;
            color: #aaa;
        }
        
        .silence-indicator {
            margin-top: 10px;
            height: 4px;
            background: #333;
            border-radius: 2px;
            overflow: hidden;
            display: none;
        }
        
        .silence-progress {
            height: 100%;
            background: #3498db;
            width: 0%;
            transition: width 0.1s linear;
        }
        
        .interrupt-note {
            position: fixed;
            bottom: 180px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0,0,0,0.8);
            padding: 10px 20px;
            border-radius: 20px;
            font-size: 14px;
            display: none;
        }
        
        .audio-visualizer {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            display: flex;
            gap: 3px;
            height: 40px;
            align-items: center;
        }
        
        .audio-bar {
            width: 4px;
            background: white;
            transition: height 0.1s;
        }
    </style>
</head>
<body>
    <div class="header">
        <h2>Smart Doc Assistant</h2>
        <div class="mode-indicator">Hands-Free Mode</div>
    </div>
    
    <div class="chat-container" id="chatContainer">
        <div class="message assistant-message">
            Hey Jeff, how's it going? Just tap the microphone when you're ready to talk. I'll stop listening after you pause.
        </div>
    </div>
    
    <div class="interrupt-note" id="interruptNote">
        Tap to interrupt
    </div>
    
    <div class="controls">
        <button class="mic-button" id="micButton" type="button">
            <span id="micIcon">🎤</span>
            <div class="audio-visualizer" id="visualizer" style="display: none;">
                <div class="audio-bar" style="height: 10px;"></div>
                <div class="audio-bar" style="height: 20px;"></div>
                <div class="audio-bar" style="height: 15px;"></div>
                <div class="audio-bar" style="height: 25px;"></div>
                <div class="audio-bar" style="height: 18px;"></div>
            </div>
        </button>
        <div class="status" id="status">Tap to speak</div>
        <div class="silence-indicator" id="silenceIndicator">
            <div class="silence-progress" id="silenceProgress"></div>
        </div>
    </div>
    
    <script>
        let mediaRecorder = null;
        let audioChunks = [];
        let isListening = false;
        let isProcessing = false;
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let currentAudio = null;
        let conversationHistory = [];
        let silenceDuration = 0;
        let resetTimeout = null;
        let recordingStream = null;
        let analysisStream = null;
        
        const micButton = document.getElementById('micButton');
        const micIcon = document.getElementById('micIcon');
        const visualizer = document.getElementById('visualizer');
        const status = document.getElementById('status');
        const chatContainer = document.getElementById('chatContainer');
        const silenceIndicator = document.getElementById('silenceIndicator');
        const silenceProgress = document.getElementById('silenceProgress');
        const interruptNote = document.getElementById('interruptNote');
        
        // Initialize audio on first button press (for iOS)
        async function initAudio() {
            try {
                if (mediaRecorder && mediaRecorder.state !== 'inactive') return;
                
                console.log('Requesting microphone...');
                status.textContent = 'Getting microphone...';
                
                // Simple audio request - PROVEN TO WORK!
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                console.log('Got media stream');
                status.textContent = 'Microphone ready';
                
                // Store the recording stream
                recordingStream = stream;
                
                // Create MediaRecorder FIRST with no format (like test page that works)
                mediaRecorder = new MediaRecorder(stream);
                console.log('MediaRecorder created with default format');
                
                mediaRecorder.ondataavailable = event => {
                    console.log('Data available:', event.data.size);
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };
                
                mediaRecorder.onstop = async () => {
                    console.log('MediaRecorder stopped, chunks:', audioChunks.length);
                    if (audioChunks.length > 0) {
                        const audioBlob = new Blob(audioChunks);
                        console.log('Created blob, size:', audioBlob.size);
                        audioChunks = [];
                        await processAudio(audioBlob);
                    } else {
                        console.log('No audio chunks recorded');
                        resetButton();
                    }
                };
                
                mediaRecorder.onerror = (event) => {
                    console.error('MediaRecorder error:', event.error);
                    resetButton();
                };
                
                // NOW setup audio context for analysis (after MediaRecorder is ready)
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    console.log('Audio context created, state:', audioContext.state);
                    
                    // Resume if needed (iOS)
                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                        console.log('Audio context resumed');
                    }
                }
                
                // Create analyser AFTER MediaRecorder
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                analyser.smoothingTimeConstant = 0.8;
                
                // Clone the stream for analysis to avoid interference
                analysisStream = stream.clone();
                microphone = audioContext.createMediaStreamSource(analysisStream);
                microphone.connect(analyser);
                console.log('Audio analysis setup with cloned stream');
                
                // Test the analyser
                setTimeout(() => {
                    const testData = new Uint8Array(analyser.frequencyBinCount);
                    analyser.getByteFrequencyData(testData);
                    const sum = testData.reduce((a, b) => a + b, 0);
                    console.log('Audio test after 100ms - sum:', sum, 'max:', Math.max(...testData));
                    
                    if (sum === 0) {
                        console.warn('Analyser returning 0 - may need fallback');
                    }
                }, 100);
                
                // Start monitoring
                monitorAudioLevels();
                console.log('Audio initialization complete');
                
            } catch (err) {
                console.error('Microphone access denied:', err);
                status.textContent = 'Microphone access required';
                micButton.disabled = true;
            }
        }
        
        function monitorAudioLevels() {
            if (!analyser) return;
            
            const dataArray = new Uint8Array(analyser.frequencyBinCount);
            const SILENCE_THRESHOLD = 30;
            const SILENCE_DURATION = 3000; // 3 seconds
            const INTERRUPT_THRESHOLD = 40;
            let debugCounter = 0;
            let analyserBroken = false;
            
            function checkAudioLevel() {
                if (!isListening && !currentAudio) {
                    requestAnimationFrame(checkAudioLevel);
                    return;
                }
                
                analyser.getByteFrequencyData(dataArray);
                const sum = dataArray.reduce((a, b) => a + b, 0);
                const average = sum / dataArray.length;
                
                // Debug every second
                debugCounter++;
                if (debugCounter % 60 === 0) {
                    console.log('Audio level - sum:', sum, 'average:', average.toFixed(2));
                }
                
                // Update visualizer when listening
                if (isListening && mediaRecorder && mediaRecorder.state === 'recording') {
                    const bars = visualizer.querySelectorAll('.audio-bar');
                    
                    // Check if analyser is working
                    if (sum === 0 && !analyserBroken) {
                        console.warn('Analyser broken - using fallback visualization');
                        analyserBroken = true;
                    }
                    
                    if (analyserBroken) {
                        // Fallback visualization - random movement
                        bars.forEach((bar) => {
                            const height = 10 + Math.random() * 20;
                            bar.style.height = height + 'px';
                        });
                    } else {
                        // Normal visualization
                        bars.forEach((bar, i) => {
                            const height = Math.min(40, dataArray[i * 50] / 3);
                            bar.style.height = height + 'px';
                        });
                    }
                    
                    // Silence detection with fallback
                    if (sum === 0 && mediaRecorder.state === 'recording') {
                        // Analyser is broken, don't auto-stop
                        silenceDuration = 0;
                        silenceProgress.style.width = '0%';
                        if (debugCounter % 120 === 0) {
                            console.log('Analyser broken - auto-stop disabled');
                        }
                    } else if (average < SILENCE_THRESHOLD) {
                        silenceDuration += 16;
                        const progress = Math.min(100, (silenceDuration / SILENCE_DURATION) * 100);
                        silenceProgress.style.width = progress + '%';
                        
                        if (silenceDuration >= SILENCE_DURATION) {
                            console.log('Silence detected, stopping...');
                            stopListening();
                        }
                    } else {
                        silenceDuration = 0;
                        silenceProgress.style.width = '0%';
                    }
                }
                
                // Check for voice interruption while audio is playing
                if (currentAudio && !currentAudio.paused && !isListening) {
                    if (average > INTERRUPT_THRESHOLD && sum > 0) {
                        console.log('Voice interruption detected');
                        currentAudio.pause();
                        currentAudio = null;
                        interruptNote.style.display = 'none';
                        status.textContent = 'Interrupted - tap to speak';
                    }
                }
                
                requestAnimationFrame(checkAudioLevel);
            }
            
            checkAudioLevel();
        }
        
        // Handle button clicks and touches
        async function handleMicPress() {
            console.log('Button pressed - isProcessing:', isProcessing, 'isListening:', isListening);
            
            if (isProcessing) {
                console.log('Blocked - still processing');
                return;
            }
            
            // Initialize audio on first press (for iOS permissions)
            if (!mediaRecorder) {
                await initAudio();
                // Give it a moment to initialize
                setTimeout(() => {
                    if (mediaRecorder) {
                        startListening();
                    }
                }, 100);
                return;
            }
            
            // Clear any pending reset
            if (resetTimeout) {
                clearTimeout(resetTimeout);
                resetTimeout = null;
            }
            
            if (currentAudio) {
                // Interrupt current audio
                currentAudio.pause();
                currentAudio = null;
                interruptNote.style.display = 'none';
                addMessage('(Interrupted)', 'system-message');
            }
            
            if (isListening) {
                console.log('Stopping...');
                stopListening();
            } else {
                console.log('Starting...');
                startListening();
            }
        }
        
        // Add event listeners
        micButton.addEventListener('click', handleMicPress);
        micButton.addEventListener('touchend', (e) => {
            e.preventDefault();
            handleMicPress();
        });
        
        function startListening() {
            console.log('startListening called - mediaRecorder:', mediaRecorder, 'state:', mediaRecorder?.state);
            
            if (!mediaRecorder) {
                console.error('MediaRecorder is null!');
                return;
            }
            
            if (mediaRecorder.state === 'inactive') {
                console.log('Starting recording...');
                audioChunks = [];
                silenceDuration = 0;
                
                // Add a beep to indicate recording started
                try {
                    const beep = new AudioContext();
                    const oscillator = beep.createOscillator();
                    oscillator.connect(beep.destination);
                    oscillator.frequency.value = 800;
                    oscillator.start();
                    oscillator.stop(beep.currentTime + 0.1);
                } catch (e) {
                    console.log('Beep failed:', e);
                }
                
                mediaRecorder.start(100);
                isListening = true;
                micButton.classList.add('listening');
                micIcon.style.display = 'none';
                visualizer.style.display = 'flex';
                status.textContent = 'Listening... (stops on pause)';
                silenceIndicator.style.display = 'block';
            }
        }
        
        function stopListening() {
            console.log('stopListening called - state:', mediaRecorder?.state);
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                isListening = false;
                micButton.classList.remove('listening');
                micButton.classList.add('processing');
                micIcon.style.display = 'block';
                micIcon.textContent = '⏳';
                visualizer.style.display = 'none';
                status.textContent = 'Processing...';
                silenceIndicator.style.display = 'none';
                silenceProgress.style.width = '0%';
                mediaRecorder.stop();
            }
        }
        
        async function processAudio(audioBlob) {
            try {
                isProcessing = true;
                console.log('Processing audio blob, size:', audioBlob.size);
                
                const formData = new FormData();
                formData.append('audio', audioBlob, 'recording.webm');
                
                const response = await fetch('/api/voice/transcribe', {
                    method: 'POST',
                    body: formData
                });
                
                const result = await response.json();
                console.log('Transcription result:', result);
                
                if (result.success && result.text) {
                    // Filter out common hallucinations
                    const text = result.text.trim();
                    const hallucinations = ['you', 'thank you', 'thanks for watching', 'bye', 'bye-bye', '.', ''];
                    
                    if (hallucinations.includes(text.toLowerCase())) {
                        status.textContent = 'No speech detected - tap to try again';
                        resetButton();
                        return;
                    }
                    
                    addMessage(text, 'user-message');
                    
                    // Show processing indicator
                    const thinkingMsg = addMessage('G is thinking...', 'system-message');
                    
                    // Get AI response
                    const aiResponse = await fetch('/api/chat', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({
                            message: text,
                            history: conversationHistory
                        })
                    });
                    
                    const aiResult = await aiResponse.json();
                    console.log('AI response:', aiResult);
                    
                    // Remove thinking message
                    if (thinkingMsg && thinkingMsg.parentNode) {
                        thinkingMsg.remove();
                    }
                    
                    if (aiResult.success && aiResult.response) {
                        conversationHistory.push({user: text, assistant: aiResult.response});
                        addMessage(aiResult.response, 'assistant-message');
                        
                        // Generate and play audio response
                        try {
                            const ttsResponse = await fetch('/api/voice/speak', {
                                method: 'POST',
                                headers: {
                                    'Content-Type': 'application/json',
                                },
                                body: JSON.stringify({
                                    text: aiResult.response
                                })
                            });
                            
                            const ttsResult = await ttsResponse.json();
                            
                            if (ttsResult.success && ttsResult.audio_url) {
                                currentAudio = new Audio(ttsResult.audio_url);
                                interruptNote.style.display = 'block';
                                
                                currentAudio.onended = () => {
                                    console.log('Audio ended - resetting states');
                                    currentAudio = null;
                                    interruptNote.style.display = 'none';
                                    resetButton();
                                    // Optional: Auto-start listening after delay
                                    // setTimeout(() => startListening(), 500);
                                };
                                
                                currentAudio.onerror = () => {
                                    console.error('Audio playback error');
                                    currentAudio = null;
                                    interruptNote.style.display = 'none';
                                    resetButton();
                                };
                                
                                await currentAudio.play();
                            } else {
                                // No audio response
                                console.log('No audio response received');
                                resetButton();
                            }
                        } catch (ttsError) {
                            console.error('TTS error:', ttsError);
                            resetButton();
                        }
                    } else {
                        status.textContent = 'Error getting response';
                        resetButton();
                    }
                } else {
                    status.textContent = 'Error: ' + (result.error || 'No transcription');
                    resetButton();
                }
            } catch (error) {
                console.error('Processing error:', error);
                status.textContent = 'Error: ' + error.message;
                resetButton();
            }
        }
        
        function resetButton() {
            console.log('Resetting button state');
            isProcessing = false;
            isListening = false;
            micButton.classList.remove('processing', 'listening');
            micIcon.style.display = 'block';
            micIcon.textContent = '🎤';
            visualizer.style.display = 'none';
            status.textContent = 'Tap to speak';
            silenceIndicator.style.display = 'none';
            silenceProgress.style.width = '0%';
            
            // Clear any pending reset
            if (resetTimeout) {
                clearTimeout(resetTimeout);
                resetTimeout = null;
            }
        }
        
        function addMessage(text, className) {
            const messageDiv = document.createElement('div');
            messageDiv.className = 'message ' + className;
            messageDiv.textContent = text;
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
            return messageDiv;
        }
        
        // Don't initialize on load - wait for first button press (iOS requirement)
        // This matches the working test page approach
        console.log('Page loaded - waiting for user interaction to init audio');
        
        // Handle page visibility changes
        document.addEventListener('visibilitychange', () => {
            if (document.hidden) {
                // Page is hidden, stop any ongoing operations
                if (isListening) {
                    stopListening();
                }
                if (currentAudio) {
                    currentAudio.pause();
                }
            }
        });
        
        // Prevent double-tap zoom on mobile
        let lastTouchEnd = 0;
        document.addEventListener('touchend', (e) => {
            const now = Date.now();
            if (now - lastTouchEnd <= 300) {
                e.preventDefault();
            }
            lastTouchEnd = now;
        }, false);
    </script>
</body>
</html>